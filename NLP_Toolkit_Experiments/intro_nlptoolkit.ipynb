{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting nltk\n",
      "  Downloading nltk-3.8.1-py3-none-any.whl (1.5 MB)\n",
      "     ---------------------------------------- 1.5/1.5 MB 19.2 MB/s eta 0:00:00\n",
      "Requirement already satisfied: click in c:\\users\\prksh\\appdata\\roaming\\python\\python310\\site-packages (from nltk) (7.1.2)\n",
      "Requirement already satisfied: joblib in c:\\users\\prksh\\appdata\\roaming\\python\\python310\\site-packages (from nltk) (1.2.0)\n",
      "Requirement already satisfied: tqdm in c:\\users\\prksh\\appdata\\roaming\\python\\python310\\site-packages (from nltk) (4.65.0)\n",
      "Collecting regex>=2021.8.3\n",
      "  Downloading regex-2023.12.25-cp310-cp310-win_amd64.whl (269 kB)\n",
      "     ------------------------------------- 269.5/269.5 kB 16.2 MB/s eta 0:00:00\n",
      "Requirement already satisfied: colorama in c:\\users\\prksh\\appdata\\roaming\\python\\python310\\site-packages (from tqdm->nltk) (0.4.4)\n",
      "Installing collected packages: regex, nltk\n",
      "Successfully installed nltk-3.8.1 regex-2023.12.25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.0.1 -> 24.0\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = \"\"\"Hello this is Satya Prakash, I am going to learn NLP, here.\n",
    "This notebook is my playgroud! and I'll be deep diving into the practical aspects of NLP.\n",
    "What are you doing today?\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello this is Satya Prakash, I am going to learn NLP, here.\n",
      "This notebook is my playgroud! and I'll be deep diving into the practical aspects of NLP.\n",
      "What are you doing today?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Tokenizations\n",
    "## sentence --> tokenize\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = sent_tokenize(corpus) # tokenize the paragraph into sentences or say 'documents'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello this is Satya Prakash, I am going to learn NLP, here.\n",
      "This notebook is my playgroud!\n",
      "and I'll be deep diving into the practical aspects of NLP.\n",
      "What are you doing today?\n"
     ]
    }
   ],
   "source": [
    "for sentence in documents:\n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello',\n",
       " 'this',\n",
       " 'is',\n",
       " 'Satya',\n",
       " 'Prakash',\n",
       " ',',\n",
       " 'I',\n",
       " 'am',\n",
       " 'going',\n",
       " 'to',\n",
       " 'learn',\n",
       " 'NLP',\n",
       " ',',\n",
       " 'here',\n",
       " '.',\n",
       " 'This',\n",
       " 'notebook',\n",
       " 'is',\n",
       " 'my',\n",
       " 'playgroud',\n",
       " '!',\n",
       " 'and',\n",
       " 'I',\n",
       " \"'ll\",\n",
       " 'be',\n",
       " 'deep',\n",
       " 'diving',\n",
       " 'into',\n",
       " 'the',\n",
       " 'practical',\n",
       " 'aspects',\n",
       " 'of',\n",
       " 'NLP',\n",
       " '.',\n",
       " 'What',\n",
       " 'are',\n",
       " 'you',\n",
       " 'doing',\n",
       " 'today',\n",
       " '?']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Tokenizations\n",
    "## sentence --> words\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "word_tokenize(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', 'this', 'is', 'Satya', 'Prakash', ',', 'I', 'am', 'going', 'to', 'learn', 'NLP', ',', 'here', '.']\n",
      "['This', 'notebook', 'is', 'my', 'playgroud', '!']\n",
      "['and', 'I', \"'ll\", 'be', 'deep', 'diving', 'into', 'the', 'practical', 'aspects', 'of', 'NLP', '.']\n",
      "['What', 'are', 'you', 'doing', 'today', '?']\n"
     ]
    }
   ],
   "source": [
    "for sentence in documents:\n",
    "    print(word_tokenize(sentence)) # tokenizes the test into words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', 'this', 'is', 'Satya', 'Prakash', ',', 'I', 'am', 'going', 'to', 'learn', 'NLP', ',', 'here', '.']\n",
      "['This', 'notebook', 'is', 'my', 'playgroud', '!']\n",
      "['and', 'I', \"'\", 'll', 'be', 'deep', 'diving', 'into', 'the', 'practical', 'aspects', 'of', 'NLP', '.']\n",
      "['What', 'are', 'you', 'doing', 'today', '?']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import wordpunct_tokenize\n",
    "\n",
    "\n",
    "\n",
    "for sentence in documents:\n",
    "    print(wordpunct_tokenize(sentence)) #  tokenizes the text into words and punctuation marks, such as commas or periods</s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "H e l l o   t h i s   i s   S a t y a   P r a k a s h,   I   a m   g o i n g   t o   l e a r n   N L P,   h e r e.\n",
      "T h i s   n o t e b o o k   i s   m y   p l a y g r o u d!\n",
      "a n d   I' l l   b e   d e e p   d i v i n g   i n t o   t h e   p r a c t i c a l   a s p e c t s   o f   N L P.\n",
      "W h a t   a r e   y o u   d o i n g   t o d a y?\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import TreebankWordDetokenizer\n",
    "\n",
    "tokenizer = TreebankWordDetokenizer()\n",
    "\n",
    "for sentence in documents:\n",
    "    print(tokenizer.tokenize(sentence)) #  tokenizes each sentence into words, preserving the original order of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Stemming\n",
    "\n",
    "## Porter Stemmer\n",
    "\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "# Sample text\n",
    "text = \"Fairly and sportingly Stemming example with the Porter stemming algorithm. This example showcases how words are reduced to their root form.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = word_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fairly----->fairli\n",
      "and----->and\n",
      "sportingly----->sportingli\n",
      "Stemming----->stem\n",
      "example----->exampl\n",
      "with----->with\n",
      "the----->the\n",
      "Porter----->porter\n",
      "stemming----->stem\n",
      "algorithm----->algorithm\n",
      ".----->.\n",
      "This----->thi\n",
      "example----->exampl\n",
      "showcases----->showcas\n",
      "how----->how\n",
      "words----->word\n",
      "are----->are\n",
      "reduced----->reduc\n",
      "to----->to\n",
      "their----->their\n",
      "root----->root\n",
      "form----->form\n",
      ".----->.\n"
     ]
    }
   ],
   "source": [
    "pstemmer = PorterStemmer()\n",
    "\n",
    "# Apply stemming to each word\n",
    "for word in words:\n",
    "    print(word+'----->'+pstemmer.stem(word))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Regexstemmer\n",
    "\n",
    "from nltk.stem import RegexpStemmer\n",
    "\n",
    "reg_stem = RegexpStemmer('ing$|s$|e$|able$', min=4)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cheat'"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg_stem.stem('cheating')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'creat'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg_stem.stem('create')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Snowball Stemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "snstemmer = SnowballStemmer('english')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fairly ==> fair\n",
      "and ==> and\n",
      "sportingly ==> sport\n",
      "Stemming ==> stem\n",
      "example ==> exampl\n",
      "with ==> with\n",
      "the ==> the\n",
      "Porter ==> porter\n",
      "stemming ==> stem\n",
      "algorithm ==> algorithm\n",
      ". ==> .\n",
      "This ==> this\n",
      "example ==> exampl\n",
      "showcases ==> showcas\n",
      "how ==> how\n",
      "words ==> word\n",
      "are ==> are\n",
      "reduced ==> reduc\n",
      "to ==> to\n",
      "their ==> their\n",
      "root ==> root\n",
      "form ==> form\n",
      ". ==> .\n"
     ]
    }
   ],
   "source": [
    "for word in words:\n",
    "    print(word+' ==> '+snstemmer.stem(word))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming cannot be efficiently with chatbots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Stemming cannot be efficiently with chatbots"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
